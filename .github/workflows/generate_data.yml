name: Continuous Data Generation
on:
  push:
    branches: [ master, main ]
  workflow_dispatch:

permissions:
  contents: write
  actions: write

jobs:
  generate:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.PAT_TOKEN || secrets.GITHUB_TOKEN }}

    - name: Download NNUE Networks
      run: |
        cd src
        wget https://tests.stockfishchess.org/api/nn/nn-c288c895ea92.nnue
        wget https://tests.stockfishchess.org/api/nn/nn-37f18f62d772.nnue

    - name: Build Engine (Linux)
      run: |
        cd src
        g++ -O3 -std=c++17 -Wall -fno-exceptions -DNDEBUG -DIS_64BIT -DUSE_PTHREADS -march=native \
        -Wno-unused-variable -Wno-unused-but-set-variable \
        benchmark.cpp bitboard.cpp evaluate.cpp main.cpp misc.cpp movegen.cpp movepick.cpp \
        position.cpp search.cpp thread.cpp timeman.cpp tt.cpp uci.cpp ucioption.cpp \
        tune.cpp syzygy/tbprobe.cpp nnue/nnue_accumulator.cpp nnue/nnue_misc.cpp \
        nnue/network.cpp nnue/features/half_ka_v2_hm.cpp nnue/features/full_threats.cpp \
        engine.cpp score.cpp memory.cpp nextfish_strategy.cpp nextfish_timeman.cpp \
        datagen.cpp -o ../nextfish -lpthread -latomic

    - name: Run Datagen (Parallel High-Quality Batch)
      run: |
        chmod +x nextfish
        python3 preprocess_pgn.py
        
        # Chạy song song 2 Core
        echo "Starting Parallel Datagen (8000 nodes)..."
        ./nextfish datagen nodes 8000 games 200 book book_moves.txt out data1.binpack &
        PID1=$!
        ./nextfish datagen nodes 8000 games 200 book book_moves.txt out data2.binpack &
        PID2=$!
        
        # Đợi 2 tiến trình kết thúc (Bản thân Engine sẽ tự gọi exit(0) khi xong)
        wait $PID1 $PID2
        
        echo "Datagen finished. Merging data..."
        cat data1.binpack data2.binpack > new_batch.binpack
        
        # Kiểm tra dung lượng và xoay file
        FILE="nextfish_data.binpack"
        CURRENT_SIZE=0
        if [ -f "$FILE" ]; then
          CURRENT_SIZE=$(stat -c%s "$FILE")
        fi
        
        NEW_SIZE=$(stat -c%s "new_batch.binpack")
        TOTAL_SIZE=$((CURRENT_SIZE + NEW_SIZE))
        
        if [ "$TOTAL_SIZE" -gt 94371840 ]; then
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          mv "$FILE" "data_${TIMESTAMP}.binpack"
          mv new_batch.binpack "$FILE"
          echo "File rotated due to size limit."
        else
          cat new_batch.binpack >> "$FILE"
          rm new_batch.binpack
        fi
        
        rm data1.binpack data2.binpack book_moves.txt

    - name: Commit & Push Data
      id: push_step
      env:
        GH_TOKEN: ${{ secrets.PAT_TOKEN || secrets.GITHUB_TOKEN }}
      run: |
        git config --global user.name 'Nextfish Data Bot'
        git config --global user.email 'bot@nextfish.ai'
        
        # Khôi phục các file hệ thống bị thay đổi/xóa nhầm (như preprocess_pgn.py nếu lỡ tay xóa)
        git checkout src/*.cpp .github/workflows/*.yml preprocess_pgn.py || true
        
        # Thêm các file binpack mới
        git add *.binpack
        
        # Kiểm tra xem có gì để commit không
        if git diff --staged --quiet; then
          echo "No new data to commit"
          exit 0
        fi

        # Commit dữ liệu
        git commit -m "Add new training data [gen]"
        
        # Đồng bộ với remote bằng rebase
        git fetch origin master
        git pull --rebase origin master
        
        # Push lên remote
        git push https://x-access-token:${GH_TOKEN}@github.com/${{ github.repository }}.git HEAD:master

    - name: Trigger Next Cycle
      if: always() && (steps.push_step.outcome == 'success' || steps.push_step.outcome == 'skipped')
      env:
        GH_TOKEN: ${{ secrets.PAT_TOKEN || secrets.GITHUB_TOKEN }}
      run: |
        echo "Waiting before triggering next run..."
        sleep 15
        gh workflow run generate_data.yml --ref ${{ github.ref_name }} || echo "Auto-trigger failed, but process may continue on next push."
